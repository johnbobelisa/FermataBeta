import cv2
import numpy as np
from ultralytics import YOLO
import os
from pathlib import Path
import json
import mediapipe as mp

class CustomHoldDetector:
    def __init__(self):
        """Initialize with your trained custom model"""
        # Path to your trained model
        self.model_path = "C:/Users/Kueh Tze Shuen/Documents/GitHub/FermataBeta/runs/detect/climbing_holds_custom/weights/best.pt"
        
        # Load the model
        try:
            self.model = YOLO(self.model_path)
            print(f"✅ Custom model loaded successfully!")
            print(f"Model classes: {self.model.names}")
        except Exception as e:
            print(f"❌ Error loading model: {e}")
            raise
    
    def detect_holds(self, image_path, confidence_threshold=0.5):
        """Detect holds in an image"""
        if not os.path.exists(image_path):
            print(f"❌ Image not found: {image_path}")
            return None, []
        
        # Load and resize image before detection
        image = cv2.imread(image_path)
        target_size = (640, 640)
        image = cv2.resize(image, target_size)

        # Run inference
        results = self.model(image, conf=confidence_threshold)

        
        holds = []
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    # Get bounding box coordinates
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0].cpu().numpy()
                    id = int(box.cls[0].cpu().numpy())
                    
                    # Calculate center and size
                    xNorm = int((x1 + x2) / 2)
                    yNorm = int((y1 + y2) / 2)
                    width = int(x2 - x1)
                    height = int(y2 - y1)
                    
                    # Get class name
                    type = self.model.names[id]
                    
                    holds.append({
                        'id': id,
                        'type': type,
                        'center_px': (xNorm, yNorm),
                        'confidence': float(confidence),
                        'bbox': (int(x1), int(y1), width, height),
                        
                        
                    })
        
        return results[0].orig_img, holds    
    
    def draw_detections(self, image, holds):
        """Draw detected holds on the image"""
        # Color mapping for different classes
        colors = {
            0: (0, 255, 0),    # Green for holds
            1: (255, 0, 0),    # Blue for volumes
        }
        
        for i, hold in enumerate(holds):
            if is_hold_color(image, hold['bbox'], target_color='green'):
                id = hold['id']
                type = hold['type']
                
                xNorm, yNorm = hold['center_px']
                confidence = hold['confidence']
                
                x, y, w, h = hold['bbox']
                
                # Choose color based on class
                color = colors.get(id, (255, 255, 255))
                
                # Draw bounding box
                cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
                
                # Draw center point
                cv2.circle(image, (xNorm, yNorm), 5, color, -1)
                
                # Draw label
                label = f"{type} {confidence:.2f}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                
                # Background for text
                cv2.rectangle(image, (x, y - label_size[1] - 10), 
                            (x + label_size[0], y), color, -1)
                
                # Text
                cv2.putText(image, label, (x, y - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                
                # Number the holds
                cv2.putText(image, str(i + 1), (xNorm - 10, yNorm + 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return image
    
    def process_image(self, image_path, save_result=True):
        """Complete processing pipeline"""
        print(f"🔍 Processing: {image_path}")
        
        # Detect holds
        original_image, holds = self.detect_holds(image_path)
        
        target_size = (640, 640)  # width, height
        original_image = cv2.resize(original_image, target_size)
        
        if original_image is None:
            return None
        
        # Draw results
        result_image = self.draw_detections(original_image.copy(), holds)
        
        # Print results
        print(f"✅ Found {len(holds)} objects:")
        
        class_counts = {}
        for hold in holds:
            type = hold['type']
            class_counts[type] = class_counts.get(type, 0) + 1
            print(f"   {hold['type']}: confidence {hold['confidence']:.3f}")
        
        print(f"\n📊 Summary:")
        for type, count in class_counts.items():
            print(f"   {type}: {count} detected")
        
        # Save result
        if save_result:
            output_path = image_path.replace('.jpg', '_detected.jpg').replace('.png', '_detected.png')
            cv2.imwrite(output_path, result_image)
            print(f"💾 Result saved: {output_path}")
        
        return result_image, holds
    
    def process_video(self, video_path, save_result=True, output_video_path="output_detected.mp4"):
        mp_pose = mp.solutions.pose
        mp_drawing = mp.solutions.drawing_utils
        pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)

        cap = cv2.VideoCapture(video_path)
        frame_num = 0
        all_holds = []

        # Get video properties for writer
        fps = cap.get(cv2.CAP_PROP_FPS)
        width, height = 640, 640  # Since you resize every frame

        # Define the codec and create VideoWriter object
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (width, height))

            # --- POSE ESTIMATION ---
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(frame_rgb)
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,255), thickness=1, circle_radius=1),
                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255,0,255), thickness=1, circle_radius=1)
                )
            # --- END POSE ESTIMATION ---

            # Run detection on the frame
            _, holds = self.detect_holds_on_frame(frame)
            all_holds.append(holds)
            # Draw only green holds
            result_image = self.draw_detections(frame.copy(), holds)
            # Write the frame to the output video
            out.write(result_image)
            frame_num += 1

        cap.release()
        out.release()
        print(f"🎬 Output video saved as {output_video_path}")
        return result_image, holds  # Return last frame's result for compatibility

    def detect_holds_on_frame(self, frame, confidence_threshold=0.5):
        """Detect holds in a single frame (already loaded image)"""
        results = self.model(frame, conf=confidence_threshold)
        holds = []
        for result in results:
            boxes = result.boxes
            if boxes is not None:
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = box.conf[0].cpu().numpy()
                    id = int(box.cls[0].cpu().numpy())
                    xNorm = int((x1 + x2) / 2)
                    yNorm = int((y1 + y2) / 2)
                    width = int(x2 - x1)
                    height = int(y2 - y1)
                    type = self.model.names[id]
                    holds.append({
                        'id': id,
                        'type': type,
                        'center_px': (xNorm, yNorm),
                        'confidence': float(confidence),
                        'bbox': (int(x1), int(y1), width, height),
                    })
        return frame, holds


def is_hold_color(image, bbox, target_color):
    # Define HSV ranges for your target color
    color_ranges = {
        'red': ([0, 100, 100], [10, 255, 255]),
        'yellow': ([20, 100, 100], [30, 255, 255]),
        'green': ([40, 40, 40], [70, 255, 255]),  # Add this line

            # Add more as needed
    }
    lower, upper = color_ranges[target_color]
    x, y, w, h = bbox
    crop = image[y:y+h, x:x+w]
    hsv = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)
    mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
    return np.count_nonzero(mask) > 0.1 * mask.size  # At least 20% pixels match





def main():
    """Test the custom detector"""
    detector = CustomHoldDetector()
    
    # Test images
    test_images = [
        # "frontend/FermataBeta/src/images/2.jpg",  # Your test image
        # "frontend/FermataBeta/src/images/images.jpeg",  # Another test image
        # "frontend/FermataBeta/src/images/image.jpg",
        "frontend/FermataBeta/src/images/IMG_5639.mov"
    ]
    
    mp_pose = mp.solutions.pose
    mp_drawing = mp.solutions.drawing_utils
    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)
    
    for image_path in test_images:
        if os.path.exists(image_path):
            print(f"\n{'='*50}")
            result_image, holds = detector.process_video(image_path)

            # --- Save only green holds as JSON ---
            filtered_holds = []
            for hold in holds:
                if is_hold_color(result_image, hold['bbox'], target_color='green'):
                    hold_copy = hold.copy()
                    if isinstance(hold_copy['center_px'], tuple):
                        hold_copy['center_px'] = list(hold_copy['center_px'])
                    if isinstance(hold_copy['bbox'], tuple):
                        hold_copy['bbox'] = list(hold_copy['bbox'])
                    filtered_holds.append(hold_copy)

            # Write to file (one file per image)
            json_filename = os.path.splitext(os.path.basename(image_path))[0] + "_green_holds.json"
            with open(json_filename, "w") as f:
                json.dump(filtered_holds, f, indent=2)
            print(f"💾 Green holds data saved to {json_filename}")
            # --- End JSON save ---

            if result_image is not None:
                # Display result
                cv2.imshow('Custom Hold Detection', result_image)
                print("Press any key to continue to next image...")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
        else:
            print(f"⚠️  Image not found: {image_path}")

if __name__ == "__main__":
    main()